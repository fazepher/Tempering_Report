[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Non-reversible Tempering for multimodal densities",
    "section": "",
    "text": "This document contains my First-Year Summer Project report within the Statistics CDT at the University of Warwick. It attempts to be a fairly concise, accesible, and reproducible exposition of the work done under the helpful supervision of Prof. Gareth O. Roberts and Dr. Nick Tawn, both of whom I thank dearly.\nIt is build through Quarto Books, allowing the reader to access it through either HTML or pdf formats."
  },
  {
    "objectID": "01_goal.html",
    "href": "01_goal.html",
    "title": "1  The Goal",
    "section": "",
    "text": "A common statistical goal is to estimate an expectated value of a function \\(h\\) of a random variable \\(X \\in \\mathcal{X}\\subset\\mathbb{R}^d\\):\n\\[\\mathbb{E}_X[h(X)] = \\int\\limits_{\\mathcal{X}} h(x) d\\pi_X.\\]\nDespite the simple expression above, these integrals are generally intractable so that one cannot compute them analytically. Furthermore, the dimension \\(d\\) is sufficiently high, as to also render numerical integration infeasable.\nOne would like then to resort to a Law of Large numbers via Monte Carlo methods. That is, if one is able to simulate in a computer a random sample \\(\\lbrace X_s\\rbrace_{s=1}^S \\overset{iid}{\\sim} \\pi_X\\), then a reasonable estimator of the target expectation is the sample average since the latter converges to the former as \\(S\\) grows:\n\\[\\hat{h}_S := \\dfrac{1}{S}\\sum\\limits_{s=1}^S h(X_s) \\underset{S\\to\\infty}{\\longrightarrow} \\mathbb{E}_X[h(X)].\\]\nThis is promising, but beyond some simpler situations, it is also hard to obtain independent samples from the distribution \\(\\pi\\). Fortunately, it may be easier to generate dependent samples and still use the sample average as an estimator. The idea is that one can construct a Markov Chain whose invariant distribution is \\(\\pi\\) and for which an Ergodic Theorem applies to justify our estimation procedure. Indeed, a Markov Chain Monte Carlo estimator, or MCMC estimator for short, is constructed by simulating a realization of a Markov Chain \\(\\lbrace X_s\\rbrace_{s=1}^S\\) such that the following is true:\n\\[\\begin{split}\n&X_s \\overset{\\mathcal{D}}{\\underset{s\\to\\infty}{\\longrightarrow}} \\pi_X \\\\\n&X_s \\sim \\pi \\Longrightarrow X_{s+1} \\sim \\pi_X \\\\\n&\\hat{h}_S \\underset{S\\to\\infty}{\\longrightarrow} \\mathbb{E}_X[h(X)]\n\\end{split}\\]\nFor illustration purposes, let us estimate the mean of a univariate standard normal by constructing the most basic type of MCMC chain: Random Walk Metropolis. The traceplot in the top-left panel of Figure 1.1 shows the evolution of the chain where the horizontal axis represents the sample number and the vertical axis the \\(x\\) state of the chain; we see a rapidly oscilating ‘catterpillar’ pattern around the bulk of the distribution centered at the true mean value of \\(0\\) signaled by the orange horizontal reference line. On the top-right panel we see the histogram approximation of the chain sample (blue) to the true density (orange); while not perfect, we can see a decent match. Most importantly to our estimation purposes, we see on the bottom panel the evolution of the ergodic averages otherwise known as the cummulative mean as the chain evolves; indeed there is convergence to the true mean.\n\n\n\n\n\nFigure 1.1: Random Walk Metropolis estimation of a univariate standard normal density.\n\n\n\n\nThis method also works for higher dimensions. If we now take a \\(20\\)-dimensional multivariate normal with independent components, we see the same pattern of convergence of the ergodic averages to each of the marginal means (top of Figure 1.2). Thinking of a more general expectation of interest, we could for example consider the probability of the first component being bigger than the second component:\n\\[\\mathbb{P}_X(X_1 \\geq X_2) = \\mathbb{E}_X\\left[\\,\\mathbb{I}(X_1 \\geq X_2)\\,\\right] = 0.5,\\]\nthe convergence is shown in the bottom of Figure 1.2.\n\n\n\n\n\nFigure 1.2: Convergence of Random Walk Metropolis ergodic averages to the marginal means of a 20-dimensional multivariate independent normal.\n\n\n\n\nOf course, more complicated distributions lead to more difficult expectations and have lead to a big and exciting area of research aiming to devise better and more efficient MCMC algorithms beyond Random Walk Metropolis. For example, Gibbs Sampling, Elliptic Slice Sampling or Hamiltonian Monte Carlo are all versions of the general Metropolis-Hastings algorithm.1 Another generalization to problems where there is change in dimensionality, like model selection, is Reversible Jump MCMC.\nWhile these methods have been highly successful, a situation where they fail badly is whenever the distribution been targeted exhibits multimodality. For example, consider the following mixture of normals in Figure 1.3.\n\n\n\n\n\nFigure 1.3: Density of a mixture of two univariate normals.\n\n\n\n\n\n\n\n\n\nIt is well documented that the growth of MCMC statistical methodology started with Gibbs Sampling and the availability of software like BUGS, which has continued and expanded to tools like JAGS, Stan, PyMC, BlackJAX or Turing↩︎"
  }
]